{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ecb8ca-541d-45f5-89f3-b93a691e71b0",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  C Linear Algebra (CLA) Library\n",
    "</h1>\n",
    "\n",
    "CLA is a simple toy library for basic vector/matrix operations in C. This project main goal is to learn the foundations of [CUDA](https://docs.nvidia.com/cuda/), and Python bindings, using [`ctypes`](https://docs.python.org/3/library/ctypes.html) as a wrapper, through simple Linear Algebra operations (additions, subtraction, multiplication, broadcasting, etc). \n",
    "\n",
    "[![Python](https://img.shields.io/pypi/pyversions/pycla.svg)](https://badge.fury.io/py/pycla)\n",
    "[![PyPI](https://badge.fury.io/py/pycla.svg)](https://badge.fury.io/py/pycla)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/moesio-f/cla/blob/main/examples/intro.ipynb)\n",
    "\n",
    "# Features\n",
    "\n",
    "- C17 support, Python 3.13, CUDA 12.8;\n",
    "- Linux support;\n",
    "- Vector-vector operations;\n",
    "- Matrix-matrix operations;\n",
    "- Vector and matrix norms;\n",
    "- GPU device selection to run operations;\n",
    "- Get CUDA information from the system (i.e., support, number of devices, etc);\n",
    "- Management of memory (CPU memory vs GPU memory), allowing copies between devices;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c987827-2414-41c9-9350-e1b03d94a4fc",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "For the C-only API, obtain the latest binaries and headers from the [releases](https://github.com/moesio-f/cla/releases) tab in GitHub. For the Python API, use your favorite package manager (i.e., `pip`, `uv`) and install `pycla` from PyPi (e.g., `pip install pycla`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9250f0c-2827-4fb4-8034-d4b34f449a48",
   "metadata": {},
   "source": [
    "### C API\n",
    "\n",
    "The C API provides structs (see [`cla/include/entities.h`](cla/include/entities.h)) and functions (see [`cla/include/vector_operations.h`](cla/include/vector_operations.h), [`cla/include/matrix_operations.h`](cla/include/matrix_operations.h)) that operate over those structs. The two main entities are `Vector` and `Matrix`. A vector or matrix can reside in either the CPU memory (host memory, from CUDA's terminology) or GPU memory (device memory). Those structs always keep metadata on the CPU (i.e., shape, current device), which allows the CPU to coordinate most of the workflow. In order for an operation to be run on the GPU the entities must first be copied to the GPU's memory.\n",
    "\n",
    "For a quickstart, compile the [samples/c_api.c](samples/c_api.c) with: (i) `gcc -l cla <filename>.c`, if you installed the library system-wide (i.e., copied the headers to `/usr/include/` and shared library to `/usr/lib/`); or (ii) `gcc -I <path-to-include> -L <path-to-root-wih-libcla> -l cla <filename>.c`. \n",
    "\n",
    "To run, make the `libcla.so` findable by the executable (i.e., either update `LD_LIBRARY_PATH` environment variable or include it on `/usr/lib`) and run in the shell of your preference (i.e., `./a.out`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51bf2e-16fe-4f58-8881-aee3d311f2fd",
   "metadata": {},
   "source": [
    "### Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4e377-254d-4dfa-ad4a-1b9e41ef2a87",
   "metadata": {},
   "source": [
    "The Python API provides an object-oriented approach for using the low-level C API. All features of the C API are exposed by the [`Vector`](pycla/core/vector.py) and [`Matrix`](pycla/core/matrix.py) classes. Some samples are available at [`samples`](samples) using Jupyter Notebooks. The code below showcases the basic features of the API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b78a5-b416-45d6-89cf-a14b58d5ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  %pip install --upgrade pip uv\n",
    "  !python -m uv pip install pycla\n",
    "except ImportError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde36bb-cb55-4cc4-92a4-324e74b1c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core entities\n",
    "from pycla import Vector, Matrix\n",
    "\n",
    "# Contexts for intensive computation\n",
    "from pycla.core import ShareDestionationVector, ShareDestionationMatrix\n",
    "\n",
    "# Vector and Matrices can be instantiated directly from Python lists/sequences\n",
    "vector = Vector([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "# e.g, for Matrices\n",
    "# matrix = Matrix([[1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "\n",
    "# Vector and Matrices can be moved forth and back to the GPU with the `.to(...)` and `.cpu()` methods\n",
    "# Once an object is on the GPU, we cannot directly read its data from the CPU,\n",
    "#    however we can still retrieve its metadata (i.e., shape, device name, etc)\n",
    "vector.to(0)\n",
    "print(vector)\n",
    "\n",
    "# We can bring an object back to the CPU with either\n",
    "#   .to(None) or .cpu() calls\n",
    "vector.cpu()\n",
    "print(vector)\n",
    "\n",
    "# The Vector class overrides the built-in operators\n",
    "#  of Python. Most of the time, the result of an operation\n",
    "#  return a new Vector instead of updating the current one\n",
    "#  in place.\n",
    "result = vector ** 2\n",
    "print(result)\n",
    "\n",
    "# We can also directly release the memory allocated\n",
    "#  for a vector with\n",
    "vector.release()\n",
    "del vector\n",
    "\n",
    "# Whenever we apply an operation on a Vector/Matrix,\n",
    "#   a new object is allocated in memory to store the result.\n",
    "# The only exception are the 'i' operations (i.e., *=, +=, -=, etc),\n",
    "#   which edit the object in place.\n",
    "# However, for some extensive computation, it is desirable to\n",
    "#   waste as little memory and time as possible. Thus, the\n",
    "#   ShareDestination{Vector,Matrix} contexts allow for using\n",
    "#   a single shared object for most operation with vectors and matrices.\n",
    "a = Vector([1.0] * 10)\n",
    "b = Vector([2.0] * 10)\n",
    "with ShareDestionationVector(a, b) as result:\n",
    "    op1 = a + b\n",
    "    op2 = result * 2\n",
    "    op3 = result / 2\n",
    "\n",
    "# All op1, op2 and op3 vectors represent the\n",
    "#  same vector.\n",
    "print(result)\n",
    "print(Vector.has_shared_data(op1, result))\n",
    "print(Vector.has_shared_data(op2, result))\n",
    "print(Vector.has_shared_data(op3, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f74c6b-248e-4fd0-abe7-ba5aa4dbc5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
